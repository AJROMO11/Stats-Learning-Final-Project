mean(data.test$GROSS.SQUARE.FEET)
median(data.test$GROSS.SQUARE.FEET)
library(caret)
partition2 <- createDataPartition(Fixed_DATA$GROSS.SQUARE.FEET, p = 0.7, list = FALSE)
data.train_F = DATA[partition2, ]
data.test_F = DATA[-partition2, ]
print("data.train_F")
mean(data.train_F$GROSS.SQUARE.FEET)
median(data.train_F$GROSS.SQUARE.FEET)
print("data.test_F")
mean(data.test_F$GROSS.SQUARE.FEET)
median(data.test_F$GROSS.SQUARE.FEET)
#This is creating a test and training data split based on a 70% 30% data split. It is also reporting the mean and median of our training and testing data.
library(caret)
partition <- createDataPartition(DATA$GROSS.SQUARE.FEET, p = 0.7, list = FALSE)
data.train = DATA[partition, ]
data.test = DATA[-partition, ]
print("data.train")
mean(data.train$GROSS.SQUARE.FEET)
median(data.train$GROSS.SQUARE.FEET)
print("data.test")
mean(data.test$GROSS.SQUARE.FEET)
median(data.test$GROSS.SQUARE.FEET)
library(caret)
partition2 <- createDataPartition(Fixed_DATA$GROSS.SQUARE.FEET, p = 0.7, list = FALSE)
data.train_F = DATA[partition2, ]
data.test_F = DATA[-partition2, ]
print("data.train_F")
mean(data.train_F$GROSS.SQUARE.FEET)
median(data.train_F$GROSS.SQUARE.FEET)
print("data.test_F")
mean(data.test_F$GROSS.SQUARE.FEET)
median(data.test_F$GROSS.SQUARE.FEET)
#This is creating a test and training data split based on a 70% 30% data split. It is also reporting the mean and median of our training and testing data.
library(caret)
partition <- createDataPartition(DATA$GROSS.SQUARE.FEET, p = 0.7, list = FALSE)
data.train = DATA[partition, ]
data.test = DATA[-partition, ]
print("data.train")
mean(data.train$GROSS.SQUARE.FEET)
median(data.train$GROSS.SQUARE.FEET)
print("data.test")
mean(data.test$GROSS.SQUARE.FEET)
median(data.test$GROSS.SQUARE.FEET)
library(caret)
partition2 <- createDataPartition(Fixed_DATA$GROSS.SQUARE.FEET, p = 0.7, list = FALSE)
data.train_F = DATA[partition2, ]
data.test_F = DATA[-partition2, ]
print("data.train_F")
mean(data.train_F$GROSS.SQUARE.FEET)
median(data.train_F$GROSS.SQUARE.FEET)
print("data.test_F")
mean(data.test_F$GROSS.SQUARE.FEET)
median(data.test_F$GROSS.SQUARE.FEET)
#This is creating a test and training data split based on a 70% 30% data split. It is also reporting the mean and median of our training and testing data.
library(caret)
partition <- createDataPartition(DATA$GROSS.SQUARE.FEET, p = 0.7, list = FALSE)
data.train = DATA[partition, ]
data.test = DATA[-partition, ]
print("data.train")
mean(data.train$GROSS.SQUARE.FEET)
median(data.train$GROSS.SQUARE.FEET)
print("data.test")
mean(data.test$GROSS.SQUARE.FEET)
median(data.test$GROSS.SQUARE.FEET)
library(caret)
partition2 <- createDataPartition(Fixed_DATA$GROSS.SQUARE.FEET, p = 0.7, list = FALSE)
data.train_F = DATA[partition2, ]
data.test_F = DATA[-partition2, ]
print("data.train_F")
mean(data.train_F$GROSS.SQUARE.FEET)
median(data.train_F$GROSS.SQUARE.FEET)
print("data.test_F")
mean(data.test_F$GROSS.SQUARE.FEET)
median(data.test_F$GROSS.SQUARE.FEET)
mean(Fixed_DATA$GROSS.SQUARE.FEET)
#This is creating a test and training data split based on a 70% 30% data split. It is also reporting the mean and median of our training and testing data.
library(caret)
partition <- createDataPartition(DATA$GROSS.SQUARE.FEET, p = 0.7, list = FALSE)
data.train = DATA[partition, ]
data.test = DATA[-partition, ]
print("data.train")
mean(data.train$GROSS.SQUARE.FEET)
median(data.train$GROSS.SQUARE.FEET)
print("data.test")
mean(data.test$GROSS.SQUARE.FEET)
median(data.test$GROSS.SQUARE.FEET)
library(caret)
partition2 <- createDataPartition(Fixed_DATA$GROSS.SQUARE.FEET, p = 0.7, list = FALSE)
data.train_F = Fixed_DATA[partition2, ]
data.test_F = Fixed_DATA[-partition2, ]
print("data.train_F")
mean(data.train_F$GROSS.SQUARE.FEET)
median(data.train_F$GROSS.SQUARE.FEET)
print("data.test_F")
mean(data.test_F$GROSS.SQUARE.FEET)
median(data.test_F$GROSS.SQUARE.FEET)
#This is creating a test and training data split based on a 70% 30% data split. It is also reporting the mean and median of our training and testing data.
library(caret)
partition <- createDataPartition(DATA$GROSS.SQUARE.FEET, p = 0.7, list = FALSE)
data.train = DATA[partition, ]
data.test = DATA[-partition, ]
print("data.train")
mean(data.train$GROSS.SQUARE.FEET)
median(data.train$GROSS.SQUARE.FEET)
print("data.test")
mean(data.test$GROSS.SQUARE.FEET)
median(data.test$GROSS.SQUARE.FEET)
# We're doing this to create a partition just to see what it would look like using the fixed data.
library(caret)
partition2 <- createDataPartition(Fixed_DATA$GROSS.SQUARE.FEET, p = 0.7, list = FALSE)
data.train_F = Fixed_DATA[partition2, ]
data.test_F = Fixed_DATA[-partition2, ]
print("data.train_F")
mean(data.train_F$GROSS.SQUARE.FEET)
median(data.train_F$GROSS.SQUARE.FEET)
print("data.test_F")
mean(data.test_F$GROSS.SQUARE.FEET)
median(data.test_F$GROSS.SQUARE.FEET)
ggplot(Fixed_DATA, aes(x = logGROSS.SQUARE.FEET)) + geom_histogram()
model.full <- lm(logGROSS.SQUARE.FEET ~ BOROUGH.COMBINED + RESIDENTIAL.UNITS + COMMERCIAL.UNITS + YEAR.BUILT + COMBINED.TAX.CLASS.SALE)
model.full <- lm(
logGROSS.SQUARE.FEET ~ BOROUGH.COMBINED + RESIDENTIAL.UNITS + COMMERCIAL.UNITS + YEAR.BUILT + COMBINED.TAX.CLASS.SALE, data = Fixed_DATA
)
summary(model.full)
round(confint(model.full, level = 0.95), 2)
# This is the linear regression for the full model.
model.full <- lm(
logGROSS.SQUARE.FEET ~ BOROUGH.COMBINED + RESIDENTIAL.UNITS + COMMERCIAL.UNITS + YEAR.BUILT + COMBINED.TAX.CLASS.SALE, data = Fixed_DATA
)
summary(model.full)
# Calculate the quantiles and the IQR based on GSF.
DATA$logGROSS.SQUARE.FEET <- log(DATA$GROSS.SQUARE.FEET)
Q1 <- quantile(DATA$logGROSS.SQUARE.FEET, 0.25)
Q3 <- quantile(DATA$logGROSS.SQUARE.FEET, 0.75)
IQR <- Q3 - Q1
print("Q1")
print(Q1)
print("Q3")
print(Q3)
print("IQR")
print(IQR)
# Now we will define the bounds of our data. If anything falls outside these bounds, we will remove them for being outliers.
lower_bound <- Q1 - 1.5 * IQR
print("lowerbound")
print(lower_bound)
upper_bound <- Q3 + 1.5 * IQR
print(upper_bound)
# Now, we will identify the outliers. We're going to omit the lower bound because we can't have negative gross square feet.
outliers <- which(DATA$logGROSS.SQUARE.FEET < lower_bound|DATA$logGROSS.SQUARE.FEET > upper_bound)
print("outliers")
outliers
#Using a different method to identify outliers
#Box_1 <- boxplot.stats(DATA$GROSS.SQUARE.FEET)
#Box_1
#Testing_Box <- boxplot.stats(DATA$GROSS.SQUARE.FEET)$out
#print("Outliers Boxplot stats")
#Testing_Box
#We've identified 24 outliers, now we need to remove them. Fixed_Data will refer to our data without these outliers in them.
Fixed_DATA <- DATA[-outliers, ]
# This is the linear regression for the full model.
model.full <- lm(
logGROSS.SQUARE.FEET ~ BOROUGH.COMBINED + RESIDENTIAL.UNITS + COMMERCIAL.UNITS + YEAR.BUILT + SALE.PRICE + COMBINED.TAX.CLASS.SALE, data = Fixed_DATA
)
summary(model.full)
for (i in vars.categorical2){
# Use the table() function to calculate the frequencies for each factor
table <- as.data.frame(table(Fixed_DATA[, i]))
# Determine the level with the highest frequency
max <- which.max(table[, 2])
# Save the name of the level with the highest frequency
level.name <- as.character(table[max, 1])
# Set the baseline level to the most populous level
Fixed_DATA2[, i] <- relevel(Fixed_DATA[, i], ref = level.name)
}
for (i in vars.categorical2){
# Use the table() function to calculate the frequencies for each factor
table <- as.data.frame(table(Fixed_DATA[, i]))
# Determine the level with the highest frequency
max <- which.max(table[, 2])
# Save the name of the level with the highest frequency
level.name <- as.character(table[max, 1])
# Set the baseline level to the most populous level
Fixed_DATA[, i] <- relevel(Fixed_DATA[, i], ref = level.name)
}
View(Fixed_DATA)
View(Fixed_DATA)
for (i in vars.categorical2){
# Use the table() function to calculate the frequencies for each factor
table <- as.data.frame(table(Fixed_DATA[, i]))
# Determine the level with the highest frequency
max <- which.max(table[, 2])
# Save the name of the level with the highest frequency
level.name <- as.character(table[max, 1])
# Set the baseline level to the most populous level
Fixed_DATA[, i] <- relevel(Fixed_DATA[, i], ref = level.name)
}
print("Dimension of the Fixed Dataset")
dim(Fixed_DATA)
print("Categorical Summary")
summary(Fixed_DATA[, vars.categorical2])
# To make sure factors in the training set are releveled
data.train_F <- Fixed_DATA[partition, ]
model.full <- lm(
logGROSS.SQUARE.FEET ~ BOROUGH.COMBINED + RESIDENTIAL.UNITS + COMMERCIAL.UNITS + YEAR.BUILT + SALE.PRICE + COMBINED.TAX.CLASS.SALE, data = Fixed_DATA
)
summary(model.full)
for (i in vars.categorical2){
# Use the table() function to calculate the frequencies for each factor
table <- as.data.frame(table(Fixed_DATA[, i]))
# Determine the level with the highest frequency
max <- which.max(table[, 2])
# Save the name of the level with the highest frequency
level.name <- as.character(table[max, 1])
# Set the baseline level to the most populous level
Fixed_DATA[, i] <- relevel(Fixed_DATA[, i], ref = level.name)
}
print("Dimension of the Fixed Dataset")
dim(Fixed_DATA)
print("Categorical Summary")
summary(Fixed_DATA[, vars.categorical2])
#Notice that Manhattan's Borough is now too low. We will have to address this later as it under 5% of the observations.
# To make sure factors in the training set are releveled
data.train_F <- Fixed_DATA[partition, ]
model.full <- lm(
logGROSS.SQUARE.FEET ~ BOROUGH.COMBINED + RESIDENTIAL.UNITS + COMMERCIAL.UNITS + YEAR.BUILT + SALE.PRICE + COMBINED.TAX.CLASS.SALE, data = Fixed_DATA
)
summary(model.full)
#Now we're binarizing these variables.
library(caret)
binarizer <- dummyVars((~ BOROUGH.COMBINED + COMBINED.TAX.CLASS.SALE),
data = Fixed_DATA, fullRank = TRUE)
binarized_vars <- data.frame(predict(binarizer, newdata = Fixed_DATA))
head(binarized_vars)
outliers <- which(DATA$logGROSS.SQUARE.FEET < lower_bound|DATA$logGROSS.SQUARE.FEET > upper_bound)
print("outliers")
head(outliers)
knitr::opts_chunk$set(echo = TRUE)
# Similar Process to what we were doing above except with a scatterplot function.
# We're now going to be trying to see if we can graphically detect any interaction effects.
# Recall:vars.numeric <- colnames(DATA[, c(5, 6, 8, 11)])
for (i in vars.numeric){
plot <- ggplot(DATA, aes(x = DATA[, i], y = GROSS.SQUARE.FEET, color = BOROUGH.COMBINED)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(x = i)
print(plot)
}
knitr::opts_chunk$set(echo = TRUE)
# We're 1st going to import our dataset. The data set is called "nyc-rolling-sales_initialcleaning(1)".
DATA <- read.csv("C:\\Users\\Daniel Scott\\Desktop\\College\\Fall 2025\\Statistical Learning\\Ordered NYC Data.csv")
# Next, we generate a summary. We're also checking how many observations we have and how many variables.
summary(DATA)
dim(DATA)
names(DATA)
# So we have 10,079 observations, and 11 variables. The names() command tells you the names.
# Now, we want to look at what our target variable looks like. Our target variable = Gross.Square.Feet
summary(DATA$GROSS.SQUARE.FEET)
library(ggplot2)
ggplot(DATA, aes(x = GROSS.SQUARE.FEET)) + geom_histogram(binwidth = 100000)
# So it looks like there may be some outliers. Clearly, something is up because these bins are really really large.The range of our data is very very very large which means it may be tough to graphically look at our data.
# By looking at our mean and median, we see that the mean is MUCH larger than our median. This indicates a positively skewed data set. Since no values of our target variable are 0, we can use a logarithmic transformation to try and make our variable more normal. Let's see how that looks now.
ggplot(DATA, aes(x = log(GROSS.SQUARE.FEET))) + geom_histogram()
# Alright. This looks waaaayyy better. It still looks positively skewed but we're doing much better.
# Now, let's look at our correlation matrix. Our numerical predictors are in columns: five, six, eight and 11. Our target variable is in column 7
cor.matrix <- cor(DATA[, c(5, 6, 8, 11, 7 )])
round(cor.matrix, digits = 3)
# The correlations for our numeric predictors are not very high, which is what we want. We don't want to have to deal with high collinearities, if I am remembering properly.
# Notice that sale price is strongly correlated with gross square feet, and residential units is also strongly correlated as well.
# First we're saving the names of the numeric predictors (N.P.) as a vector. Then we're plotting each N.P. against Gross.Square.Feet.
vars.numeric <- colnames(DATA[, c(5, 6, 8, 11)])
for (i in vars.numeric){
plot <- ggplot(DATA, aes(x = DATA[, i], y = GROSS.SQUARE.FEET)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(x = i)
print(plot)
}
# We see that all of the N.P.'s, except year built have a positive correlation with gross square feet.
# We see that year built has almost a 0 correlation with gross square feet. So we'll keep that in the back of our mind.
library(tidyverse)
# Now, we're going to save the names of our categorical predictors (CP) as a vector. The column numbers that correspond to our CP's are one through four, then nine and ten.
# Then our function will go ahead and generate the mean and median of Gross Square Feet (GSF) for each level of each CP.
vars.categorical <- colnames(DATA[, c(1:4, 9:10)])
for (i in vars.categorical) {
x <- DATA %>%
group_by_(i) %>%
summarise(
mean = mean(GROSS.SQUARE.FEET),
median = median(GROSS.SQUARE.FEET),
n = n()
)
print(x)
}
# A couple of notes:
# Most of our CP's have some levels with small n's that we may want to combine to reduce dimensionality.
# There are some levels that seem to make a large difference in the mean and median of GSF.
# So we're going to need to consider the 5% rule, which states that each factor level should contain at least 5% of the total observations. For our data set, n = 10,079. 0.05*10,079 = 503.95, which we will round up to 504. So each factor level must contain at least 504 observations in it.
# Let's start doing that here.
# We're just going to combine Borough two, which is the Bronx with Borough four, which is Queens because they both border the East River in NYC.We're checking that everything looks good with the factor count function.
library(forcats)
F_Borough <- factor(x = DATA$BOROUGH)
fct_count(F_Borough)
# We've created a factor for Borough. Now we want to combine levels 2 and 4. Then we'll make sure it worked with the fct_count function again.
F_Borough_Comb <- fct_collapse(F_Borough, "2+4"=c(2, 4))
fct_count(F_Borough_Comb, prop = TRUE)
# So we've created a new factor called "Factor Borough Combined" where we combined the Bronx and Queens. Now we just need to add it to our data set.
DATA$BOROUGH.COMBINED <- F_Borough_Comb
# For our next variable "Neighborhood", we're going to want to combine many factor levels together. This will be tricky. I am thinking that we will need to use some logic to combine our factor levels together. Probably we will do it based on keep adding factor levels together until the combined factor level has at least 504 observations in it. Time to figure out how to do this.
# First we need to load the forcats library.
#library(forcats)
#F_Neighborhood <- factor(x = DATA$NEIGHBORHOOD)
# Next, we are going to have a count of each ordered Neighborhood.
#Neigh_Count <- fct_count(DATA$NEIGHBORHOOD)
#print(Neigh_Count)
# I am first going to sort the neighborhoods in order they appear in our data set. This is defined as "Ordered_Neighborhood."
#Ordered_Neighborhood <- fct_inorder(DATA$NEIGHBORHOOD)
#print(Ordered_Neighborhood)
# Let's check what the factor count of ordered Neighborhood looks like.
#fct_count(Ordered_Neighborhood)
# Ok this is what we wanted.
# Let's try to now combine this factor NOT based on geographic location but based on Zip Code. This will effectively allow us to include zip code's information and have some sembelence related to geographic information included. Note: Zip Codes are much more about post offices and less to do with geographic information necessarily.
#fct_reorder2(F_Neighborhood, DATA$NEIGHBORHOOD, DATA$ZIP.CODE)
#F_ZipCode <- factor(DATA$ZIP.CODE)
#fct_count(F_ZipCode)
#We're going to come back to this later. We are going to skip these high dimensionality categorical variables and instead just run what we can at the moment.
# First, let's look at Tax Class At Time of Sale.
Tax_Clss_Sale <- factor(x = DATA$TAX.CLASS.AT.TIME.OF.SALE)
fct_count(Tax_Clss_Sale)
# We see that this gives us one level of tax class at time of sale that is under 504. This is a little bit worrisome as the information included may potentially mess with whatever factor level it is included in. Let's combine it with factor level 2, as they have a similar median. We will do this by using the
Tax_Clss_Sale_Comb <- fct_collapse(Tax_Clss_Sale, "2+4" = c(2, 4) )
fct_count(f = Tax_Clss_Sale_Comb)
DATA$COMBINED.TAX.CLASS.SALE <-Tax_Clss_Sale_Comb
# Now I want to see how this affected the mean and median of the different groups.
library(tidyverse)
# This first function will show us the mean and median of the Tax Class Variable.
by_Tax_Clss_Sale <- DATA %>%
group_by(TAX.CLASS.AT.TIME.OF.SALE) %>%
summarise(
mean = mean(GROSS.SQUARE.FEET),
median = median(GROSS.SQUARE.FEET)
)
print(by_Tax_Clss_Sale)
# This function will show us the mean and median of the Combined Tax Class Variable.
by_Tax_Clss_Sale_Combined <- DATA %>%
group_by(COMBINED.TAX.CLASS.SALE) %>%
summarise(
mean = mean(GROSS.SQUARE.FEET),
median = median(GROSS.SQUARE.FEET)
)
print(by_Tax_Clss_Sale_Combined)
# We see that the mean changes for both groups significantly but the median remains the same!
# So we've combined levels 2 and 4 of tax class sale and have put it into our data set.
library(tidyverse)
# First, we're going to save the names of our categorical predictors (CP) as a vector. The column numbers that correspond to our CP's are column 12 and column 13.
# Then our function will go ahead and generate the mean and median of Gross Square Feet (GSF) for each level of each CP.
vars.categorical2 <- c("COMBINED.TAX.CLASS.SALE", "BOROUGH.COMBINED")
for (i in vars.categorical2) {
x2 <- DATA %>%
group_by_(i) %>%
summarise(
mean = mean(GROSS.SQUARE.FEET),
median = median(GROSS.SQUARE.FEET),
n = n()
)
print(x2)
}
# We see a couple of things. 1st: Combined Tax Class Sale seems to have a pretty sizeable impact on GSF. 2nd: Borough Combined also seems to have a noticeable impact on GSF.
# Let's look at some graphical displays as well.
for (i in vars.categorical2){
cat.plot <- ggplot(DATA, aes(x = DATA[, i], y = GROSS.SQUARE.FEET)) +
geom_boxplot(outlier.color = "red") +
labs(x = i)
print(cat.plot)
}
# HMMM.... This is making me think that there are definetly some outliers in our data set. Let's see what happens if we remove these outliers.
for (i in vars.categorical2){
cat.plot <- ggplot(DATA, aes(x = DATA[, i], y = GROSS.SQUARE.FEET)) +
geom_boxplot(outliers = FALSE) +
labs(x = i)
print(cat.plot)
}
# Clearly there's some effect that both of these are having on GSF.
# Calculate the quantiles and the IQR based on GSF.
DATA$logGROSS.SQUARE.FEET <- log(DATA$GROSS.SQUARE.FEET)
Q1 <- quantile(DATA$logGROSS.SQUARE.FEET, 0.25)
Q3 <- quantile(DATA$logGROSS.SQUARE.FEET, 0.75)
IQR <- Q3 - Q1
print("Q1")
print(Q1)
print("Q3")
print(Q3)
print("IQR")
print(IQR)
# Now we will define the bounds of our data. If anything falls outside these bounds, we will remove them for being outliers.
lower_bound <- Q1 - 1.5 * IQR
print("lowerbound")
print(lower_bound)
upper_bound <- Q3 + 1.5 * IQR
print(upper_bound)
# Now, we will identify the outliers. We're going to omit the lower bound because we can't have negative gross square feet.
outliers <- which(DATA$logGROSS.SQUARE.FEET < lower_bound|DATA$logGROSS.SQUARE.FEET > upper_bound)
#Using a different method to identify outliers
#Box_1 <- boxplot.stats(DATA$GROSS.SQUARE.FEET)
#Box_1
#Testing_Box <- boxplot.stats(DATA$GROSS.SQUARE.FEET)$out
#print("Outliers Boxplot stats")
#Testing_Box
#We've identified 24 outliers, now we need to remove them. Fixed_Data will refer to our data without these outliers in them.
Fixed_DATA <- DATA[-outliers, ]
# Similar Process to what we were doing above except with a scatterplot function.
# We're now going to be trying to see if we can graphically detect any interaction effects.
# Recall:vars.numeric <- colnames(DATA[, c(5, 6, 8, 11)])
for (i in vars.numeric){
plot <- ggplot(DATA, aes(x = DATA[, i], y = GROSS.SQUARE.FEET, color = BOROUGH.COMBINED)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(x = i)
print(plot)
}
# Residential Units doesn't appear to interact with Borough
# Commercial units may interact with Borough
# Something odd is happening with year built... Particularly with Borough 1. Those outliers appear to be skewing things drastically.
# Sale Price may interact with borough.
# I think these provide more evidence of outliers causing problems in our data set.
#We will have to address them.
#This is creating a test and training data split based on a 70% 30% data split. It is also reporting the mean and median of our training and testing data.
library(caret)
partition <- createDataPartition(DATA$GROSS.SQUARE.FEET, p = 0.7, list = FALSE)
data.train = DATA[partition, ]
data.test = DATA[-partition, ]
print("data.train")
mean(data.train$GROSS.SQUARE.FEET)
median(data.train$GROSS.SQUARE.FEET)
print("data.test")
mean(data.test$GROSS.SQUARE.FEET)
median(data.test$GROSS.SQUARE.FEET)
# We're doing this to create a partition just to see what it would look like using the fixed data.
library(caret)
partition2 <- createDataPartition(Fixed_DATA$GROSS.SQUARE.FEET, p = 0.7, list = FALSE)
data.train_F = Fixed_DATA[partition2, ]
data.test_F = Fixed_DATA[-partition2, ]
print("data.train_F")
mean(data.train_F$GROSS.SQUARE.FEET)
median(data.train_F$GROSS.SQUARE.FEET)
print("data.test_F")
mean(data.test_F$GROSS.SQUARE.FEET)
median(data.test_F$GROSS.SQUARE.FEET)
#The means are much closer, so I would recommend using the second data set.
#Just making a histogram to see what our logarithmic model look like with our fixed data.
ggplot(Fixed_DATA, aes(x = logGROSS.SQUARE.FEET)) + geom_histogram()
#This looks much better.
# This is the linear regression for the full model.
model.full <- lm(
logGROSS.SQUARE.FEET ~ BOROUGH.COMBINED + RESIDENTIAL.UNITS + COMMERCIAL.UNITS + YEAR.BUILT + SALE.PRICE + COMBINED.TAX.CLASS.SALE, data = Fixed_DATA
)
summary(model.full)
round(confint(model.full, level = 0.95), 2)
for (i in vars.categorical2){
# Use the table() function to calculate the frequencies for each factor
table <- as.data.frame(table(Fixed_DATA[, i]))
# Determine the level with the highest frequency
max <- which.max(table[, 2])
# Save the name of the level with the highest frequency
level.name <- as.character(table[max, 1])
# Set the baseline level to the most populous level
Fixed_DATA[, i] <- relevel(Fixed_DATA[, i], ref = level.name)
}
print("Dimension of the Fixed Dataset")
dim(Fixed_DATA)
print("Categorical Summary")
summary(Fixed_DATA[, vars.categorical2])
#Notice that Manhattan's Borough is now too low. We will have to address this later as it under 5% of the observations.
# To make sure factors in the training set are releveled
data.train_F <- Fixed_DATA[partition, ]
model.full <- lm(
logGROSS.SQUARE.FEET ~ BOROUGH.COMBINED + RESIDENTIAL.UNITS + COMMERCIAL.UNITS + YEAR.BUILT + SALE.PRICE + COMBINED.TAX.CLASS.SALE, data = Fixed_DATA
)
summary(model.full)
#Now we're binarizing these variables.
library(caret)
binarizer <- dummyVars((~ BOROUGH.COMBINED + COMBINED.TAX.CLASS.SALE),
data = Fixed_DATA, fullRank = TRUE)
binarized_vars <- data.frame(predict(binarizer, newdata = Fixed_DATA))
head(binarized_vars)
Data.bin <- cbind()
Data.bin <- cbind(Fixed_DATA, binarized_vars)
head(Data.bin)
Data.bin$BOROUGH <- NULL
Data.bin$BOROUGH.COMBINED <- NULL
Data.bin$COMBINED.TAX.CLASS.SALE <- NULL
Data.bin$BOROUGH <- NULL
Data.bin$BOROUGH.COMBINED <- NULL
Data.bin$COMBINED.TAX.CLASS.SALE <- NULL
head(Data.bin)
data.train.bin <- Data.bin[partition, ]
data.test.bin <- Data.bin[-partition, ]
#Now we will refit the MLR with our binarization
model.full.bin <- lm(
logGROSS.SQUARE.FEET ~ BOROUGH.COMBINED.1 + BOROUGH.COMBINED.2.4 + RESIDENTIAL.UNITS + COMMERCIAL.UNITS + YEAR.BUILT + SALE.PRICE + COMBINED.TAX.CLASS.SALE.2.4, data = data.train.bin
)
summary(model.full.bin)
head(binarized_vars)
head(Data.bin)
summary(model.full)
install.packages("MASS")
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(MASS)
model.backward.AIC <- stepAIC(model.full.bin)
print("MODEL SUMMARY")
summary(model.backward.AIC)
