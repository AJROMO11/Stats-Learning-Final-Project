#Ordered_Neighborhood <- fct_inorder(DATA$NEIGHBORHOOD)
#print(Ordered_Neighborhood)
# Let's check what the factor count of ordered Neighborhood looks like.
#fct_count(Ordered_Neighborhood)
# Ok this is what we wanted.
# Let's try to now combine this factor NOT based on geographic location but based on Zip Code. This will effectively allow us to include zip code's information and have some sembelence related to geographic information included. Note: Zip Codes are much more about post offices and less to do with geographic information necessarily.
#fct_reorder2(F_Neighborhood, DATA$NEIGHBORHOOD, DATA$ZIP.CODE)
#F_ZipCode <- factor(DATA$ZIP.CODE)
#fct_count(F_ZipCode)
#We're going to come back to this later. We are going to skip these high dimensionality categorical variables and instead just run what we can at the moment.
# First, let's look at Tax Class At Time of Sale.
Tax_Clss_Sale <- factor(x = DATA$TAX.CLASS.AT.TIME.OF.SALE)
fct_count(Tax_Clss_Sale)
# We see that this gives us one level of tax class at time of sale that is under 504. This is a little bit worrisome as the information included may potentially mess with whatever factor level it is included in. Let's combine it with factor level 2, as they have a similar median. We will do this by using the
Tax_Clss_Sale_Comb <- fct_collapse(Tax_Clss_Sale, "2+4" = c(2, 4) )
fct_count(f = Tax_Clss_Sale_Comb)
DATA$COMBINED.TAX.CLASS.SALE <-Tax_Clss_Sale_Comb
# Now I want to see how this affected the mean and median of the different groups.
library(tidyverse)
# This first function will show us the mean and median of the Tax Class Variable.
by_Tax_Clss_Sale <- DATA %>%
group_by(TAX.CLASS.AT.TIME.OF.SALE) %>%
summarise(
mean = mean(GROSS.SQUARE.FEET),
median = median(GROSS.SQUARE.FEET)
)
print(by_Tax_Clss_Sale)
# This function will show us the mean and median of the Combined Tax Class Variable.
by_Tax_Clss_Sale_Combined <- DATA %>%
group_by(COMBINED.TAX.CLASS.SALE) %>%
summarise(
mean = mean(GROSS.SQUARE.FEET),
median = median(GROSS.SQUARE.FEET)
)
print(by_Tax_Clss_Sale_Combined)
# We see that the mean changes for both groups significantly but the median remains the same!
# So we've combined levels 2 and 4 of tax class sale and have put it into our data set.
library(tidyverse)
# First, we're going to save the names of our categorical predictors (CP) as a vector. The column numbers that correspond to our CP's are column 12 and column 13.
# Then our function will go ahead and generate the mean and median of Gross Square Feet (GSF) for each level of each CP.
vars.categorical2 <- c("COMBINED.TAX.CLASS.SALE", "BOROUGH.COMBINED")
for (i in vars.categorical2) {
x2 <- DATA %>%
group_by_(i) %>%
summarise(
mean = mean(GROSS.SQUARE.FEET),
median = median(GROSS.SQUARE.FEET),
n = n()
)
print(x2)
}
# We see a couple of things. 1st: Combined Tax Class Sale seems to have a pretty sizeable impact on GSF. 2nd: Borough Combined also seems to have a noticeable impact on GSF.
# Let's look at some graphical displays as well.
for (i in vars.categorical2){
cat.plot <- ggplot(DATA, aes(x = DATA[, i], y = GROSS.SQUARE.FEET)) +
geom_boxplot(outlier.color = "red") +
labs(x = i)
print(cat.plot)
}
# HMMM.... This is making me think that there are definetly some outliers in our data set. Let's see what happens if we remove these outliers.
for (i in vars.categorical2){
cat.plot <- ggplot(DATA, aes(x = DATA[, i], y = GROSS.SQUARE.FEET)) +
geom_boxplot(outliers = FALSE) +
labs(x = i)
print(cat.plot)
}
# Clearly there's some effect that both of these are having on GSF.
# Calculate the quantiles and the IQR based on GSF.
DATA$logGROSS.SQUARE.FEET <- log(DATA$GROSS.SQUARE.FEET)
Q1 <- quantile(DATA$logGROSS.SQUARE.FEET, 0.25)
Q3 <- quantile(DATA$logGROSS.SQUARE.FEET, 0.75)
IQR <- Q3 - Q1
print("Q1")
print(Q1)
print("Q3")
print(Q3)
print("IQR")
print(IQR)
# Now we will define the bounds of our data. If anything falls outside these bounds, we will remove them for being outliers.
lower_bound <- Q1 - 1.5 * IQR
print("lowerbound")
print(lower_bound)
upper_bound <- Q3 + 1.5 * IQR
print(upper_bound)
# Now, we will identify the outliers. We're going to omit the lower bound because we can't have negative gross square feet.
outliers <- which(DATA$logGROSS.SQUARE.FEET < lower_bound|DATA$logGROSS.SQUARE.FEET > upper_bound)
#Using a different method to identify outliers
#Box_1 <- boxplot.stats(DATA$GROSS.SQUARE.FEET)
#Box_1
#Testing_Box <- boxplot.stats(DATA$GROSS.SQUARE.FEET)$out
#print("Outliers Boxplot stats")
#Testing_Box
#We've identified 24 outliers, now we need to remove them. Fixed_Data will refer to our data without these outliers in them.
Fixed_DATA <- DATA[-outliers, ]
# Similar Process to what we were doing above except with a scatterplot function.
# We're now going to be trying to see if we can graphically detect any interaction effects.
# Recall:vars.numeric <- colnames(DATA[, c(5, 6, 8, 11)])
for (i in vars.numeric){
plot <- ggplot(DATA, aes(x = DATA[, i], y = GROSS.SQUARE.FEET, color = BOROUGH.COMBINED)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(x = i)
print(plot)
}
# Residential Units doesn't appear to interact with Borough
# Commercial units may interact with Borough
# Something odd is happening with year built... Particularly with Borough 1. Those outliers appear to be skewing things drastically.
# Sale Price may interact with borough.
# I think these provide more evidence of outliers causing problems in our data set.
#We will have to address them.
#This is creating a test and training data split based on a 70% 30% data split. It is also reporting the mean and median of our training and testing data.
library(caret)
partition <- createDataPartition(DATA$GROSS.SQUARE.FEET, p = 0.7, list = FALSE)
data.train = DATA[partition, ]
data.test = DATA[-partition, ]
print("data.train")
mean(data.train$GROSS.SQUARE.FEET)
median(data.train$GROSS.SQUARE.FEET)
print("data.test")
mean(data.test$GROSS.SQUARE.FEET)
median(data.test$GROSS.SQUARE.FEET)
# We're doing this to create a partition just to see what it would look like using the fixed data.
library(caret)
partition2 <- createDataPartition(Fixed_DATA$GROSS.SQUARE.FEET, p = 0.7, list = FALSE)
data.train_F = Fixed_DATA[partition2, ]
data.test_F = Fixed_DATA[-partition2, ]
print("data.train_F")
mean(data.train_F$GROSS.SQUARE.FEET)
median(data.train_F$GROSS.SQUARE.FEET)
print("data.test_F")
mean(data.test_F$GROSS.SQUARE.FEET)
median(data.test_F$GROSS.SQUARE.FEET)
#The means are much closer, so I would recommend using the second data set.
#Just making a histogram to see what our logarithmic model look like with our fixed data.
ggplot(Fixed_DATA, aes(x = logGROSS.SQUARE.FEET)) + geom_histogram()
#This looks much better.
# This is the linear regression for the full model.
model.full <- lm(
logGROSS.SQUARE.FEET ~ BOROUGH.COMBINED + RESIDENTIAL.UNITS + COMMERCIAL.UNITS + YEAR.BUILT + SALE.PRICE + COMBINED.TAX.CLASS.SALE, data = Fixed_DATA
)
summary(model.full)
round(confint(model.full, level = 0.95), 2)
for (i in vars.categorical2){
# Use the table() function to calculate the frequencies for each factor
table <- as.data.frame(table(Fixed_DATA[, i]))
# Determine the level with the highest frequency
max <- which.max(table[, 2])
# Save the name of the level with the highest frequency
level.name <- as.character(table[max, 1])
# Set the baseline level to the most populous level
Fixed_DATA[, i] <- relevel(Fixed_DATA[, i], ref = level.name)
}
print("Dimension of the Fixed Dataset")
dim(Fixed_DATA)
print("Categorical Summary")
summary(Fixed_DATA[, vars.categorical2])
#Notice that Manhattan's Borough is now too low. We will have to address this later as it under 5% of the observations.
# To make sure factors in the training set are releveled
data.train_F <- Fixed_DATA[partition, ]
model.full <- lm(
logGROSS.SQUARE.FEET ~ BOROUGH.COMBINED + RESIDENTIAL.UNITS + COMMERCIAL.UNITS + YEAR.BUILT + SALE.PRICE + COMBINED.TAX.CLASS.SALE, data = Fixed_DATA
)
summary(model.full)
#Now we're binarizing these variables.
library(caret)
binarizer <- dummyVars((~ BOROUGH.COMBINED + COMBINED.TAX.CLASS.SALE),
data = Fixed_DATA, fullRank = TRUE)
binarized_vars <- data.frame(predict(binarizer, newdata = Fixed_DATA))
head(binarized_vars)
Data.bin <- cbind(Fixed_DATA, binarized_vars)
head(Data.bin)
# Clearing duplicate variables because of the binarization of borough.combined and combined.tax.class.sale
Data.bin$BOROUGH <- NULL
Data.bin$BOROUGH.COMBINED <- NULL
Data.bin$COMBINED.TAX.CLASS.SALE <- NULL
head(Data.bin)
data.train.bin <- Data.bin[partition, ]
data.test.bin <- Data.bin[-partition, ]
#Now we will refit the MLR with our binarization
model.full.bin <- lm(
logGROSS.SQUARE.FEET ~ BOROUGH.COMBINED.1 + BOROUGH.COMBINED.2.4 + RESIDENTIAL.UNITS + COMMERCIAL.UNITS + YEAR.BUILT + SALE.PRICE + COMBINED.TAX.CLASS.SALE.2.4, data = data.train.bin
)
summary(model.full.bin)
#This showws us a couple of things: Boroughs 2 & 4 are not statistically significant, as well as 2&4 tax class do not add anything significant to our model in comparison to our new baseline conditions. (For reference, baselines are borough 5 and tax class 1)
library(MASS)
model.backward.AIC <- stepAIC(model.full.bin)
print("MODEL SUMMARY")
summary(model.backward.AIC)
#basic tree
treeModel <- rpart(
GROSS.SQUARE.FEET ~ BOROUGH.COMBINED + RESIDENTIAL.UNITS + COMMERCIAL.UNITS + YEAR.BUILT + SALE.PRICE + COMBINED.TAX.CLASS.SALE,
data = Fixed_Data,
method = "anova"
)
#basic tree
treeModel <- rpart(
GROSS.SQUARE.FEET ~ BOROUGH.COMBINED + RESIDENTIAL.UNITS + COMMERCIAL.UNITS + YEAR.BUILT + SALE.PRICE + COMBINED.TAX.CLASS.SALE,
data = Fixed_DATA,
method = "anova"
)
print(treeModel)
rpart.plot(
treeModel,
extra = 1,
main = "Decision Tree Predicting Gross Square Feet"
)
#pruned decision tree
treeModel_prune <- rpart(
GROSS.SQUARE.FEET ~ BOROUGH.COMBINED + RESIDENTIAL.UNITS + COMMERCIAL.UNITS + YEAR.BUILT + SALE.PRICE + COMBINED.TAX.CLASS.SALE,
data = data.train_F,
method = "anova",
control = rpart.control(
minsplit = 20,
minbucket = 30,
cp = 0.01,
maxdepth = 5
)
)
print(treeModel_prune)
rpart.plot(
treeModel_prune,
extra = 1,
main = "Expanded and pruned tree model"
)
#basic tree
treeModel <- rpart(
GROSS.SQUARE.FEET ~ BOROUGH.COMBINED + RESIDENTIAL.UNITS + COMMERCIAL.UNITS + YEAR.BUILT + SALE.PRICE + COMBINED.TAX.CLASS.SALE,
data = Fixed_DATA,
method = "anova"
)
print(treeModel)
rpart.plot(
treeModel,
extra = 1,
main = "Decision Tree Predicting Gross Square Feet"
)
# Just note that node 11 has only 86 observations.
#pruned decision tree
treeModel_prune <- rpart(
GROSS.SQUARE.FEET ~ BOROUGH.COMBINED + RESIDENTIAL.UNITS + COMMERCIAL.UNITS + YEAR.BUILT + SALE.PRICE + COMBINED.TAX.CLASS.SALE,
data = data.train_F,
method = "anova",
control = rpart.control(
minsplit = 20,
minbucket = 30,
cp = 0.01,
maxdepth = 5
)
)
print(treeModel_prune)
rpart.plot(
treeModel_prune,
extra = 1,
main = "Expanded and pruned tree model"
)
#Calculating RMSE of our expanded and pruned decision tree
library(Metrics)
pred_rpart_log <- predict(treeModel, newdata = data.test_F)
pred_rpart_orig <- exp(pred_rpart_log)
rmse_rpart_orig <- rmse(test$GROSS.SQUARE.FEET, pred_rpart_orig)
#Calculating RMSE of our expanded and pruned decision tree
library(Metrics)
pred_rpart_log <- predict(treeModel, newdata = data.test_F)
pred_rpart_orig <- exp(pred_rpart_log)
rmse_rpart_orig <- rmse(data.test_F$GROSS.SQUARE.FEET, pred_rpart_orig)
#Calculating RMSE of our expanded and pruned decision tree
library(Metrics)
pred_rpart_log <- predict(treeModel, newdata = data.test_F)
pred_rpart_orig <- exp(pred_rpart_log)
rmse_rpart_orig <- rmse(data.test_F$GROSS.SQUARE.FEET, pred_rpart_orig)
cat("RMSE of Decision Tree: ", round(rmse_rpart_orig, 2), "\n")
#Calculating RMSE of our expanded and pruned decision tree
library(Metrics)
pred_rpart_log <- predict(treeModel_prune, newdata = data.test_F)
pred_rpart_orig <- exp(pred_rpart_log)
rmse_rpart_orig <- rmse(data.test_F$GROSS.SQUARE.FEET, pred_rpart_orig)
cat("RMSE of Decision Tree: ", round(rmse_rpart_orig, 2), "\n")
library(Metrics)
pred_rpart_log <- predict(treeModel, newdata = test)
#Calculating RMSE of our expanded and pruned decision tree
library(Metrics)
pred_rpart_log <- predict(treeModel_prune, newdata = data.test_F)
pred_rpart_orig <- exp(pred_rpart_log)
rmse_rpart_orig <- rmse(data.test_F$GROSS.SQUARE.FEET, pred_rpart_orig)
cat("RMSE of Decision Tree: ", round(rmse_rpart_orig, 2), "\n")
cat("RMSE of Decision Tree: ", round(rmse_rpart_orig, 2), "\n")
library(rpart)
library(rpart.plot)
df <- read.csv("nyc-rolling-sales_initalcleaning.csv")
#convert cat vars to factors
df$BOROUGH <- factor(df$BOROUGH)
df$NEIGHBORHOOD <- factor(df$NEIGHBORHOOD)
df$BUILDING.CLASS.CATEGORY <- factor(df$BUILDING.CLASS.CATEGORY)
df$ZIP.CODE <- factor(df$ZIP.CODE)
df$TAX.CLASS.AT.TIME.OF.SALE <- factor(df$TAX.CLASS.AT.TIME.OF.SALE)
df$BUILDING.CLASS.AT.TIME.OF.SALE <- factor(df$BUILDING.CLASS.AT.TIME.OF.SALE)
df$GROSS.SQUARE.FEET <- as.numeric(df$GROSS.SQUARE.FEET)
#basic tree
treeModel <- rpart(
GROSS.SQUARE.FEET ~ .,
data = df,
method = "anova"
)
print(treeModel)
rpart.plot(
treeModel,
extra = 1,
main = "Decision Tree Predicting Gross Square Feet"
)
#tree came out very weird with too many factors. I am trying to condense factor levels
#showing how many factor levels in each categorical variable
sapply(df, function(x) if(is.factor(x)) length(levels(x)))
#factor level proportion of observations, looking to condense so each factor is at lest 5% of observations
lapply(df[, sapply(df, is.factor)], function(x) {
prop.table(table(x))*100
})
library(forcats)
#condense building class at time of sale
df$BUILDING.CLASS.AT.TIME.OF.SALE <-
substr(as.character(df$BUILDING.CLASS.AT.TIME.OF.SALE), 1, 1)
df$BUILDING.CLASS.AT.TIME.OF.SALE <- factor(df$BUILDING.CLASS.AT.TIME.OF.SALE)
df$BUILDING.CLASS.AT.TIME.OF.SALE <-
fct_lump_prop(df$BUILDING.CLASS.AT.TIME.OF.SALE, prop = 0.02)
#condense building class
df$BUILDING.CLASS.CATEGORY <- as.character(df$BUILDING.CLASS.CATEGORY)
df$BUILDING.CLASS.CATEGORY[
grepl("RENTAL", df$BUILDING.CLASS.CATEGORY, ignore.case = TRUE)
] <- "RENTALS"
df$BUILDING.CLASS.CATEGORY <- factor(df$BUILDING.CLASS.CATEGORY)
df$BUILDING.CLASS.CATEGORY <-
fct_lump_prop(df$BUILDING.CLASS.CATEGORY, prop = 0.02)
#borough condense
levels(df$BOROUGH)[levels(df$BOROUGH) %in% c("2", "4")] <- "4"
#https://thewanderlover.com/zip-code-ny-new-york/ link for proof of zip code clusters
#zipcodes
df$ZIP.CODE <- substr(as.character(df$ZIP.CODE), 1, 3)
df$ZIP.CODE <- factor(df$ZIP.CODE)
df$ZIP.CODE <-
fct_lump_prop(df$ZIP.CODE, prop = 0.02)
#neighborhoods is too complicated to condense for this dataset so i dropped it
df$NEIGHBORHOOD <- NULL
#reconfirming that my condensing worked
sapply(df, function(x) if(is.factor(x)) length(levels(x)))
lapply(df[, sapply(df, is.factor)], function(x) {
prop.table(table(x))*100
})
df$GROSS.SQUARE.FEET <-  log(df$GROSS.SQUARE.FEET)
df$SALE.PRICE <-  log(df$SALE.PRICE)
trainIndex <- sample(seq_len(nrow(df)), size = 0.7*nrow(df))
train <- df[trainIndex, ]
test  <- df[-trainIndex, ]
#pruned decision tree
treeModel <- rpart(
GROSS.SQUARE.FEET ~ .,
data = train,
method = "anova",
control = rpart.control(
minsplit = 20,
minbucket = 30,
cp = 0.01,
maxdepth = 5
)
)
print(treeModel)
rpart.plot(
treeModel,
extra = 1,
main = "Pruned Decision Tree Predicting Gross Square Feet",
box.palette = "Blues",
shadow.col = "gray",
)
library(Metrics)
pred_rpart_log <- predict(treeModel, newdata = test)
pred_rpart_orig <- exp(pred_rpart_log)
rmse_rpart_orig <- rmse(test$GROSS.SQUARE.FEET, pred_rpart_orig)
cat("RMSE of Decision Tree: ", round(rmse_rpart_orig, 2), "\n")
#boosted tree
library(gbm)
boostModel <- gbm(
formula = GROSS.SQUARE.FEET ~ .,
data = train,
distribution = "gaussian",
n.trees = 1000,
interaction.depth = 5,
shrinkage = 0.01,
n.minobsinnode = 20,
cv.folds = 5,
verbose = FALSE
)
#best iteration
bestIter <- gbm.perf(boostModel, method = "cv")
bestIter
summary(boostModel, n.trees = bestIter)
df$predictedSqFt <- predict(boostModel, newdata = df, n.trees = bestIter)
pred_boost_log <- predict(boostModel, newdata = test, n.trees = bestIter)
pred_boost_orig <- exp(pred_boost_log)
rmse_boost_orig <- rmse(test$GROSS.SQUARE.FEET, pred_boost_orig)
cat("RMSE of Boosted Tree: ", round(rmse_boost_orig, 2), "\n")
#Calculating RMSE of our expanded and pruned decision tree
library(Metrics)
pruned_predict <- predict(treeModel_prune, newdata = data.test_F)
pruned_actual <- data.test_F$GROSS.SQUARE.FEET
RMSE_prune <- rmse(pruned_actual, pruned_predict)
print(RMSE_prune)
cat("RMSE of Decision Tree: ", round(rmse_rpart_orig, 2), "\n")
library(Metrics)
pruned_predict <- predict(treeModel_prune, newdata = data.test_F)
pruned_actual <- data.test_F$GROSS.SQUARE.FEET
RMSE_prune <- rmse(pruned_actual, pruned_predict)
print(RMSE_prune)
#Calculating RMSE of our expanded and pruned decision tree
library(Metrics)
pruned_predict <- predict(treeModel_prune, newdata = data.test_F)
pruned_actual <- data.test_F$GROSS.SQUARE.FEET
RMSE_prune <- rmse(pruned_actual, pruned_predict)
print("RMSE of Expanded and pruned tree model")
print(RMSE_prune)
#RMSE of our basic tree
library(Metrics)
basic_predict <- predict(treeModel, newdata = data.test_F)
#basic tree
treeModel <- rpart(
GROSS.SQUARE.FEET ~ BOROUGH.COMBINED + RESIDENTIAL.UNITS + COMMERCIAL.UNITS + YEAR.BUILT + SALE.PRICE + COMBINED.TAX.CLASS.SALE,
data = data.test_F,
method = "anova"
)
print(treeModel)
rpart.plot(
treeModel,
extra = 1,
main = "Decision Tree Predicting Gross Square Feet"
)
# Just note that node 11 has only 86 observations.
#RMSE of our basic tree
library(Metrics)
basic_predict <- predict(treeModel, newdata = data.test_F)
basic_actual <- data.test_F$GROSS.SQUARE.FEET
RMSE_prune <- rmse(basic_actual, basic_predict)
print("RMSE of Expanded and pruned tree model")
print(RMSE_prune)
cat("RMSE of Decision Tree: ", round(rmse_rpart_orig, 2), "\n")
#pruned decision tree
treeModel_prune <- rpart(
GROSS.SQUARE.FEET ~ BOROUGH.COMBINED + RESIDENTIAL.UNITS + COMMERCIAL.UNITS + YEAR.BUILT + SALE.PRICE + COMBINED.TAX.CLASS.SALE,
data = data.train_F,
method = "anova",
control = rpart.control(
minsplit = 20,
minbucket = 30,
cp = 0.01,
maxdepth = 5
)
)
print(treeModel_prune)
rpart.plot(
treeModel_prune,
extra = 1,
main = "Expanded and pruned tree model"
)
#Calculating RMSE of our expanded and pruned decision tree
library(Metrics)
pruned_predict <- predict(treeModel_prune, newdata = data.test_F)
pruned_actual <- data.test_F$GROSS.SQUARE.FEET
RMSE_prune <- rmse(pruned_actual, pruned_predict)
print("RMSE of Expanded and pruned tree model")
print(RMSE_prune)
#basic tree
treeModel <- rpart(
GROSS.SQUARE.FEET ~ BOROUGH.COMBINED + RESIDENTIAL.UNITS + COMMERCIAL.UNITS + YEAR.BUILT + SALE.PRICE + COMBINED.TAX.CLASS.SALE,
data = data.test_F,
method = "anova"
)
print(treeModel)
rpart.plot(
treeModel,
extra = 1,
main = "Decision Tree Predicting Gross Square Feet"
)
# Just note that node 11 has only 86 observations.
#RMSE of our basic tree
library(Metrics)
basic_predict <- predict(treeModel, newdata = data.test_F)
basic_actual <- data.test_F$GROSS.SQUARE.FEET
RMSE_prune <- rmse(basic_actual, basic_predict)
print("RMSE of basic tree model")
print(RMSE_prune)
cat("RMSE of Decision Tree: ", round(rmse_rpart_orig, 2), "\n")
#pruned decision tree
treeModel_prune <- rpart(
GROSS.SQUARE.FEET ~ BOROUGH.COMBINED + RESIDENTIAL.UNITS + COMMERCIAL.UNITS + YEAR.BUILT + SALE.PRICE + COMBINED.TAX.CLASS.SALE,
data = data.train_F,
method = "anova",
control = rpart.control(
minsplit = 20,
minbucket = 30,
cp = 0.01,
maxdepth = 5
)
)
print(treeModel_prune)
rpart.plot(
treeModel_prune,
extra = 1,
main = "Expanded and pruned tree model"
)
#Calculating RMSE of our expanded and pruned decision tree
library(Metrics)
pruned_predict <- predict(treeModel_prune, newdata = data.test_F)
pruned_actual <- data.test_F$GROSS.SQUARE.FEET
RMSE_prune <- rmse(pruned_actual, pruned_predict)
print("RMSE of Expanded and pruned tree model")
print(RMSE_prune)
