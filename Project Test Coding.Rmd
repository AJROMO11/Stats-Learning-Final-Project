---
title: "Project Test Coding"
author: "Daniel Scott"
date: "2025-10-15"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
Question (Q): What is the problem we're investigating?
  We're investigating how the size of properties in New York is predicted by
  various variables. These variables are: which neighborhood the property is in, the tax class, the building classification
  at the time of sale, the zip code of the property, the month the property was sold in, the number of residential and
  commercial units, the sales price, and the year the property was built.
  
Q: Are we focused on making predictions or are we trying to describe the relationship between
multiple variables?
  We're focused on helping people with budgets in mind understand what size of property they can buy.
  So I would say we're worried about making predictions.

Q: What kind of statistical learning are we performing?
  We want to perform Predictive Statistical learning. We're more worried about accuracy than interpretability.
  We want to lower our bias and increase our variance.

Q: What sort of analyses do we intend to perform?
  We're doing a GLM to help us predict how our variables impact the Gross Square Feet of units.
  

```{r}
# We're 1st going to import our dataset. The data set is called "nyc-rolling-sales_initialcleaning(1)".
DATA <- read.csv("C:\\Users\\Daniel Scott\\Desktop\\College\\Fall 2025\\Statistical Learning\\Ordered NYC Data.csv")

# Next, we generate a summary. We're also checking how many observations we have and how many variables. 
summary(DATA)
dim(DATA)
names(DATA)
# So we have 10,079 observations, and 11 variables. The names() command tells you the names.

# Now, we want to look at what our target variable looks like. Our target variable = Gross.Square.Feet
summary(DATA$GROSS.SQUARE.FEET)
library(ggplot2)
ggplot(DATA, aes(x = GROSS.SQUARE.FEET)) + geom_histogram(binwidth = 100000)

# So it looks like there may be some outliers. Clearly, something is up because these bins are really really large.The range of our data is very very very large which means it may be tough to graphically look at our data.

# By looking at our mean and median, we see that the mean is MUCH larger than our median. This indicates a positively skewed data set. Since no values of our target variable are 0, we can use a logarithmic transformation to try and make our variable more normal. Let's see how that looks now.
ggplot(DATA, aes(x = log(GROSS.SQUARE.FEET))) + geom_histogram()
# Alright. This looks waaaayyy better. It still looks positively skewed but we're doing much better.

# Now, let's look at our correlation matrix. Our numerical predictors are in columns: five, six, eight and 11. Our target variable is in column 7
cor.matrix <- cor(DATA[, c(5, 6, 8, 11, 7 )])
round(cor.matrix, digits = 3)
# The correlations for our numeric predictors are not very high, which is what we want. We don't want to have to deal with high collinearities, if I am remembering properly.
# Notice that sale price is strongly correlated with gross square feet, and residential units is also strongly correlated as well.
```
We're now moving onto visualizing the relationships between our numeric predictors and gross square feet.
```{r}
# First we're saving the names of the numeric predictors (N.P.) as a vector. Then we're plotting each N.P. against Gross.Square.Feet.
vars.numeric <- colnames(DATA[, c(5, 6, 8, 11)])
for (i in vars.numeric){
plot <- ggplot(DATA, aes(x = DATA[, i], y = GROSS.SQUARE.FEET)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(x = i)
print(plot)
}
# We see that all of the N.P.'s, except year built have a positive correlation with gross square feet.
# We see that year built has almost a 0 correlation with gross square feet. So we'll keep that in the back of our mind.
```
Now we are moving onto exploring our categorical variables. We have six categorical variables: borough, neighborhood, building class category, zip code, tax class, and building class at time of sale.

We're going to calculate the mean and median of our target variable (TV), which is Gross Square Feet. This will be split by the different levels of each of our categorical variables.
First we need to install tidyverse.
#```{r}
#install.packages("tidyverse")
#```
Now that it is installed, we can move onto actually using it.
```{r}
library(tidyverse)
# Now, we're going to save the names of our categorical predictors (CP) as a vector. The column numbers that correspond to our CP's are one through four, then nine and ten.
# Then our function will go ahead and generate the mean and median of Gross Square Feet (GSF) for each level of each CP.
vars.categorical <- colnames(DATA[, c(1:4, 9:10)])
for (i in vars.categorical) {
x <- DATA %>%
group_by_(i) %>%
summarise(
mean = mean(GROSS.SQUARE.FEET),
median = median(GROSS.SQUARE.FEET),
n = n()
)
print(x)
}
# A couple of notes:
# Most of our CP's have some levels with small n's that we may want to combine to reduce dimensionality.
# There are some levels that seem to make a large difference in the mean and median of GSF.

# So we're going to need to consider the 5% rule, which states that each factor level should contain at least 5% of the total observations. For our data set, n = 10,079. 0.05*10,079 = 503.95, which we will round up to 504. So each factor level must contain at least 504 observations in it.
# Let's start doing that here.

# We're just going to combine Borough two, which is the Bronx with Borough four, which is Queens because they both border the East River in NYC.We're checking that everything looks good with the factor count function.
library(forcats)
F_Borough <- factor(x = DATA$BOROUGH)
fct_count(F_Borough)


# We've created a factor for Borough. Now we want to combine levels 2 and 4. Then we'll make sure it worked with the fct_count function again.
F_Borough_Comb <- fct_collapse(F_Borough, "2+4"=c(2, 4))
fct_count(F_Borough_Comb, prop = TRUE)
# So we've created a new factor called "Factor Borough Combined" where we combined the Bronx and Queens. Now we just need to add it to our data set.
DATA$BOROUGH.COMBINED <- F_Borough_Comb


# For our next variable "Neighborhood", we're going to want to combine many factor levels together. This will be tricky. I am thinking that we will need to use some logic to combine our factor levels together. Probably we will do it based on keep adding factor levels together until the combined factor level has at least 504 observations in it. Time to figure out how to do this.
```

Ok so we need to condense the neighborhoods. Let's do it in a systematic manner. I am referring to https://locality.nyc for a map of New York City.
I am proposing we condense these levels into clusters of neighborhoods that are geographically close to one another. The first step is to choose a starting point. I am starting with the neighborhoods in Manhattan.
```{r}
# First we need to load the forcats library.
#library(forcats)
#F_Neighborhood <- factor(x = DATA$NEIGHBORHOOD)
# Next, we are going to have a count of each ordered Neighborhood.
#Neigh_Count <- fct_count(DATA$NEIGHBORHOOD)
#print(Neigh_Count)

# I am first going to sort the neighborhoods in order they appear in our data set. This is defined as "Ordered_Neighborhood."
#Ordered_Neighborhood <- fct_inorder(DATA$NEIGHBORHOOD)
#print(Ordered_Neighborhood)

# Let's check what the factor count of ordered Neighborhood looks like.
#fct_count(Ordered_Neighborhood)

# Ok this is what we wanted.
# Let's try to now combine this factor NOT based on geographic location but based on Zip Code. This will effectively allow us to include zip code's information and have some sembelence related to geographic information included. Note: Zip Codes are much more about post offices and less to do with geographic information necessarily.

#fct_reorder2(F_Neighborhood, DATA$NEIGHBORHOOD, DATA$ZIP.CODE)

#F_ZipCode <- factor(DATA$ZIP.CODE)
#fct_count(F_ZipCode)

#We're going to come back to this later. We are going to skip these high dimensionality categorical variables and instead just run what we can at the moment.

```

Ok, so we're skipping the neighborhood variable ATM for the sake of time. We're also going to be skipping over: building class category, zip code, building class at time of sale. This means we have the variables: residential units, commerical units, year, sales price, combined borough, and tax class at time of sale.

This means we need to do some work on tax class at time of sale.
```{r}
# First, let's look at Tax Class At Time of Sale.
Tax_Clss_Sale <- factor(x = DATA$TAX.CLASS.AT.TIME.OF.SALE)
fct_count(Tax_Clss_Sale)

# We see that this gives us one level of tax class at time of sale that is under 504. This is a little bit worrisome as the information included may potentially mess with whatever factor level it is included in. Let's combine it with factor level 2, as they have a similar median. We will do this by using the
Tax_Clss_Sale_Comb <- fct_collapse(Tax_Clss_Sale, "2+4" = c(2, 4) )
fct_count(f = Tax_Clss_Sale_Comb)
DATA$COMBINED.TAX.CLASS.SALE <-Tax_Clss_Sale_Comb
# Now I want to see how this affected the mean and median of the different groups.
library(tidyverse)

# This first function will show us the mean and median of the Tax Class Variable.
by_Tax_Clss_Sale <- DATA %>% 
group_by(TAX.CLASS.AT.TIME.OF.SALE) %>%
summarise(
  mean = mean(GROSS.SQUARE.FEET),
  median = median(GROSS.SQUARE.FEET)
)
print(by_Tax_Clss_Sale)

# This function will show us the mean and median of the Combined Tax Class Variable.
by_Tax_Clss_Sale_Combined <- DATA %>% 
group_by(COMBINED.TAX.CLASS.SALE) %>%
summarise(
  mean = mean(GROSS.SQUARE.FEET),
  median = median(GROSS.SQUARE.FEET)
)
print(by_Tax_Clss_Sale_Combined)

# We see that the mean changes for both groups significantly but the median remains the same!
# So we've combined levels 2 and 4 of tax class sale and have put it into our data set.
```

Now, we have our variables ready for some analyses. We're going to rerun one of our codes from earlier to help us assess if our, now two, categorical variables will have an impact on GSF.
```{r}
library(tidyverse)
# First, we're going to save the names of our categorical predictors (CP) as a vector. The column numbers that correspond to our CP's are column 12 and column 13.
# Then our function will go ahead and generate the mean and median of Gross Square Feet (GSF) for each level of each CP.
vars.categorical2 <- c("COMBINED.TAX.CLASS.SALE", "BOROUGH.COMBINED")
for (i in vars.categorical2) {
x2 <- DATA %>%
group_by_(i) %>%
summarise(
mean = mean(GROSS.SQUARE.FEET),
median = median(GROSS.SQUARE.FEET),
n = n()
)
print(x2)
}
# We see a couple of things. 1st: Combined Tax Class Sale seems to have a pretty sizeable impact on GSF. 2nd: Borough Combined also seems to have a noticeable impact on GSF.

# Let's look at some graphical displays as well.
for (i in vars.categorical2){
  cat.plot <- ggplot(DATA, aes(x = DATA[, i], y = GROSS.SQUARE.FEET)) +
    geom_boxplot(outlier.color = "red") +
    labs(x = i)
  print(cat.plot)
}
# HMMM.... This is making me think that there are definetly some outliers in our data set. Let's see what happens if we remove these outliers.
for (i in vars.categorical2){
  cat.plot <- ggplot(DATA, aes(x = DATA[, i], y = GROSS.SQUARE.FEET)) +
    geom_boxplot(outliers = FALSE) +
    labs(x = i)
  print(cat.plot)
}
# Clearly there's some effect that both of these are having on GSF.
```
We need a way to address our outliers.
```{r}
# Calculate the quantiles and the IQR based on GSF.
DATA$logGROSS.SQUARE.FEET <- log(DATA$GROSS.SQUARE.FEET)
Q1 <- quantile(DATA$logGROSS.SQUARE.FEET, 0.25)
Q3 <- quantile(DATA$logGROSS.SQUARE.FEET, 0.75)
IQR <- Q3 - Q1
print("Q1")
print(Q1)
print("Q3")
print(Q3)
print("IQR")
print(IQR)

# Now we will define the bounds of our data. If anything falls outside these bounds, we will remove them for being outliers.
lower_bound <- Q1 - 1.5 * IQR
print("lowerbound")
print(lower_bound)
upper_bound <- Q3 + 1.5 * IQR
print(upper_bound)

# Now, we will identify the outliers. We're going to omit the lower bound because we can't have negative gross square feet.
outliers <- which(DATA$logGROSS.SQUARE.FEET < lower_bound|DATA$logGROSS.SQUARE.FEET > upper_bound)

#Using a different method to identify outliers
#Box_1 <- boxplot.stats(DATA$GROSS.SQUARE.FEET)
#Box_1
#Testing_Box <- boxplot.stats(DATA$GROSS.SQUARE.FEET)$out
#print("Outliers Boxplot stats")
#Testing_Box

#We've identified 24 outliers, now we need to remove them. Fixed_Data will refer to our data without these outliers in them.
Fixed_DATA <- DATA[-outliers, ]
```
Let's make some scatterplots for our numerical variables.
```{r}
# Similar Process to what we were doing above except with a scatterplot function.
# We're now going to be trying to see if we can graphically detect any interaction effects.
# Recall:vars.numeric <- colnames(DATA[, c(5, 6, 8, 11)]) 

for (i in vars.numeric){
plot <- ggplot(DATA, aes(x = DATA[, i], y = GROSS.SQUARE.FEET, color = BOROUGH.COMBINED)) +
geom_point() +
geom_smooth(method = "lm", se = FALSE) +
labs(x = i)
print(plot)
}


# Residential Units doesn't appear to interact with Borough
# Commercial units may interact with Borough
# Something odd is happening with year built... Particularly with Borough 1. Those outliers appear to be skewing things drastically.
# Sale Price may interact with borough.
# I think these provide more evidence of outliers causing problems in our data set.
#We will have to address them.
```
Here we address the outliers. We are going to remove them for the time being.

Now we can finally, initially split our data into the test and training data sets.
```{r}
#This is creating a test and training data split based on a 70% 30% data split. It is also reporting the mean and median of our training and testing data.
library(caret)
partition <- createDataPartition(DATA$GROSS.SQUARE.FEET, p = 0.7, list = FALSE)
data.train = DATA[partition, ]
data.test = DATA[-partition, ]
print("data.train")
mean(data.train$GROSS.SQUARE.FEET)
median(data.train$GROSS.SQUARE.FEET)

print("data.test")
mean(data.test$GROSS.SQUARE.FEET)
median(data.test$GROSS.SQUARE.FEET)


# We're doing this to create a partition just to see what it would look like using the fixed data.
library(caret)
partition2 <- createDataPartition(Fixed_DATA$GROSS.SQUARE.FEET, p = 0.7, list = FALSE)
data.train_F = Fixed_DATA[partition2, ]
data.test_F = Fixed_DATA[-partition2, ]
print("data.train_F")
mean(data.train_F$GROSS.SQUARE.FEET)
median(data.train_F$GROSS.SQUARE.FEET)

print("data.test_F")
mean(data.test_F$GROSS.SQUARE.FEET)
median(data.test_F$GROSS.SQUARE.FEET)
#The means are much closer, so I would recommend using the second data set.

#Just making a histogram to see what our logarithmic model look like with our fixed data.
ggplot(Fixed_DATA, aes(x = logGROSS.SQUARE.FEET)) + geom_histogram()
#This looks much better.

```
Now we move onto to doing a Multiple Linear Regression.
```{r}
# This is the linear regression for the full model.
model.full <- lm(
  logGROSS.SQUARE.FEET ~ BOROUGH.COMBINED + RESIDENTIAL.UNITS + COMMERCIAL.UNITS + YEAR.BUILT + SALE.PRICE + COMBINED.TAX.CLASS.SALE, data = Fixed_DATA
  )
summary(model.full)
```
Here we see that the only non-significant predictor is the tax class at time of sale. Year built doesn't seem to be adding much useful information to our data though.
```{r}
round(confint(model.full, level = 0.95), 2)
```
Now we need to relevel our factors and binarize.
```{r}
for (i in vars.categorical2){
# Use the table() function to calculate the frequencies for each factor
table <- as.data.frame(table(Fixed_DATA[, i]))
# Determine the level with the highest frequency
max <- which.max(table[, 2])
# Save the name of the level with the highest frequency
level.name <- as.character(table[max, 1])
# Set the baseline level to the most populous level
Fixed_DATA[, i] <- relevel(Fixed_DATA[, i], ref = level.name)
}
print("Dimension of the Fixed Dataset")
dim(Fixed_DATA)
print("Categorical Summary")
summary(Fixed_DATA[, vars.categorical2])
#Notice that Manhattan's Borough is now too low. We will have to address this later as it under 5% of the observations.
```

```{r}
# To make sure factors in the training set are releveled
data.train_F <- Fixed_DATA[partition, ]
model.full <- lm(
  logGROSS.SQUARE.FEET ~ BOROUGH.COMBINED + RESIDENTIAL.UNITS + COMMERCIAL.UNITS + YEAR.BUILT + SALE.PRICE + COMBINED.TAX.CLASS.SALE, data = Fixed_DATA
  )
summary(model.full)

#Now we're binarizing these variables.
library(caret)
binarizer <- dummyVars((~ BOROUGH.COMBINED + COMBINED.TAX.CLASS.SALE),
data = Fixed_DATA, fullRank = TRUE)
binarized_vars <- data.frame(predict(binarizer, newdata = Fixed_DATA))
head(binarized_vars)

Data.bin <- cbind()
```